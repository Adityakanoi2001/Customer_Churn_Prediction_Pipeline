# Data Validation Stage â€“ Detailed Documentation

This document describes the purpose, workflow, and implementation details of the `data_validation.py` script in the Customer Churn Prediction Pipeline. A Mermaid flowchart is included to visualize the data validation process.

---

## Purpose

The `data_validation.py` script ensures that the ingested data is clean, consistent, and suitable for further processing. It checks for data quality issues such as missing values, duplicates, invalid data types, and out-of-range values. This step is crucial to prevent errors and unreliable results in downstream stages.

---

## Workflow Steps

1. **Load Ingested Data:**  
   - Reads the processed data file (e.g., `data/processed/ingested_data.csv`) generated by the ingestion stage.

2. **Check for Missing Values:**  
   - Identifies columns or rows with missing or null values.
   - Logs the count and location of missing data.

3. **Validate Data Types:**  
   - Ensures each column has the expected data type (e.g., numeric, categorical).
   - Logs any mismatches or conversion errors.

4. **Check for Duplicates:**  
   - Detects duplicate rows based on unique identifiers or all columns.
   - Logs the number of duplicates found and optionally removes them.

5. **Validate Value Ranges and Categories:**  
   - Checks that numerical columns fall within expected ranges.
   - Validates that categorical columns contain only allowed values.

6. **Generate Validation Report:**  
   - Summarizes all findings (issues, warnings, and data quality statistics).
   - Saves the report to a log file or prints to console.

7. **Output Cleaned Data (Optional):**  
   - Optionally outputs a cleaned version of the data for downstream use.

---

## Mermaid Flowchart

```mermaid
flowchart TD
    A[Start Validation] --> B[Load ingested data]
    B --> C[Check for missing values]
    C --> D[Validate data types]
    D --> E[Check for duplicates]
    E --> F[Validate value ranges & categories]
    F --> G[Generate validation report]
    G --> H{Critical issues?}
    H -- Yes --> I[Log error & halt/raise warning] --> J[Output cleaned data (optional)]
    H -- No --> J
    J --> K[Finish]
```

---

## Inputs

- Ingested data file (e.g., `data/processed/ingested_data.csv`)

## Outputs

- Validation report/log (e.g., `logs/validation_report.txt`)
- Optionally, cleaned data file for further processing

---

## Notes

- The script is designed to catch and log all data quality issues before modeling.
- Critical issues may halt the pipeline or raise warnings for manual review.
- Ensures that only high-quality data is passed to the next stage.

---

## Example Usage

```bash
python src/data_validation.py
```

---

This validation stage is essential for maintaining data integrity and reliability throughout the pipeline.